---
layout: post
title: 'Adjoint method I: Neural ODE'
date: 2020-06-25
permalink: /posts/2020/06/25/adjoint1/
tags: 
  - Automatic Differentiation
  - Deep Learning
  - Ordinary Differential Equation
---

Deep learning (DL) problems often end up with optimizing

$$\min_{\theta}L(x(\theta), y)$$

where we consider $y$ as some already known labels, and $x(\theta)$ is the neural network we used to predict those labels. The standard way to deal with this problem is to use backpropagation to compute the gradient of $L$ w.r.t $\theta$, and then use state of the art gradient based optimization method to find $\theta$. The question left for us becomes how our neural network should be like. Among all the designing rules, an often effective one is to make your neural network deeper, we believe that the deeper the network is, the more powerful it would be. However, as we go deeper, the number of parameters increase drastically and make it hard for us to train the model. Thus, another guiding principle we follow is to share weight. We can pack a set of layers inside a block, and reuse that block multiple times in our model. An example of this is the famous ResNet. 

Later, researchers noticed the deep connect between ordinary differential equation (ODE) and ResNet, and propose several ODE inspired neural network structures. In their seminal neural ODE paper, Tianqi Chen et. al. not only revealing these connections, they also introduce an efficent method for computing gradients of ODE based neural network, which is the adjoint method.

Adjoint method has long been known for differential equation community. It is used to evaluate the gradient of $L(x(T;\theta))$ w.r.t $\theta$ (much like what we do in DL), for simplicity, we consider the case where $L$ only implicitly dependent on $\theta$ and $x(T;\theta)$ is the solution of an ODE:

$$\frac{dx(t;\theta)}{dt}=f(x,t,\theta)\qquad x(0)=\text{const},\,\theta(t)=\theta(0)$$


We can use chain rule to evaluate the gradient. Following the Euler's method $x(t+\epsilon;\theta)=x(t;\theta)+f(x(t;\theta),t,\theta)\epsilon$, we can expand the gradient as:

$$\begin{equarray}
\frac{dL}{d\theta}\big|_{t+\epsilon} & = & \frac{dL}{dx(t+\epsilon;\theta)}\frac{dx(t+\epsilon;\theta))}{d\theta}\\
& = & \frac{dL}{dx(t+\epsilon;\theta)}\left(\frac{dx(t+\epsilon;\theta)}{dx(t;\theta)}\frac{dx(t;\theta)}{d\theta}+\frac{\partial f(x,t;\theta)}{\partial\theta}\right)\\
& = & \frac{dL}{d\theta}\big|_{t}+\epsilon\frac{dL}{dx(t+\epsilon;\theta)}\frac{\partial f(x,t;\theta)}{\partial\theta}
\end{eqnarray}$$

where $\frac{dL}{d\theta}\big|_{t}\equiv\frac{dL(x(t;\theta),t)}{d\theta}$. This provide us with a new ODE of $d_{\theta}L(t)$, and evaluate it from $0\to T$:

$$\frac{dL}{d\theta}\big|_{t=T}=\frac{dL}{d\theta}\big|_{t=0}+\int_0^T\frac{dL}{dx(t;θ)}\frac{∂ f(x,t;θ)}{∂θ}\mathbf{d}t$$

There is still one thing left inside the above equation, the $d_xL$. Using the above mentioned method, we can also find an ODE for it:

$$\begin{eqnarray}
\frac{dL}{dx(t;\theta)} & = & \frac{dL}{dx(t+\epsilon;\theta)}\frac{dx(t+\epsilon;\theta)}{dx(t;\theta)}\\
& = &  \frac{dL}{dx(t;\theta)} + \epsilon\frac{dL}{dx(t+\epsilon;\theta)}\frac{\partial f(x,t;\theta)}{\partial x(t;\theta)}
\end{eqnarray}$$

thus

$$\frac{dL}{dx(t;\theta)}=\frac{dL}{dx(T;\theta)}-\int_T^t\frac{dL}{dx(\tau;\theta)}\frac{\partial f(x,\tau;\theta)}{\partial x}\mathbf{d}\tau$$

and note that this ODE has to be evaluated in the oppsite direction.

To summarize, neural ODE provide us a way to integrate traditional differential equation solver to DL, the backpropagation through the ODE solver can be achieved by the adjoint method. 

Some takeaway:

$$\begin{eqnarray}
\frac{dL}{d\theta}\big|_{t=T} & = & \int_0^T\frac{dL}{dx(t;\theta)}\frac{\partial f(x,t;\theta)}{\parital\theta}\mathbf{d}t\\
\text{where} & & \frac{dL}{dx(t;\theta)}=\frac{dL}{dx(T;\theta)}-\int_T^t\frac{dL}{dx(\tau;\theta)}\frac{\partial f(x,\tau;\theta)}{\partial x}\mathbf{d}\tau\\
& & x(t;\theta)=x(0)+\int_0^tf(x,\tau;\theta)\mathbf{d}\tau
\end{eqnarray}$$

## Reference
[1] [Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations](https://arxiv.org/abs/1710.10121)

[2] [Neural Ordinary Differential Equation](https://arxiv.org/pdf/1806.07366.pdf)

[3] [Deep Residue Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)

